from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings 
from langchain.schema import Document
from duckduckgo_search import DDGS

# Repertoire o√π sont stock√©s les vecteurs 
CHROMA_DIR= "chroma_db"


# Mod√®le d'embeddings par d√©faut
EMBEDDING_MODEL = "nomic-embed-text"

def documentSearch(query: str, k: int = 5) -> str : 
    """
    Effectue une recherche vectorielle sur la base ChromaDB pour la requ√™te donn√©e.
    
    :param query: La requ√™te de l'utilisateur (en langage naturel)
    :param k: Le nombre de documents les plus pertinents √† retourner
    :return: Texte format√© des documents trouv√©s
    """

    # Cr√©ation embedder (via Ollama ou un autre mod√®le) 
    embedding = OllamaEmbeddings(model=EMBEDDING_MODEL)

    # Chargement base vectorielle
    vectordb = Chroma(
        persist_directory=CHROMA_DIR,
        embedding_function=embedding
    )

    # lancement recherc h de k documents les plus pertinents 
    results = vectordb.similarity_search(query, k=k)

    # Formattage r√©sultats pour un retour clair 
    if not results:
        return "Aucun document trouv√©"
    
    formatted = []

    for i, doc in enumerate(results, 1):
        formatted.append(
            f"üîπ R√©sultat {i}:\n{doc.page_content[:500]}...\n(Source: {doc.metadata.get('source_file', 'inconnu')})\n"
        )
    
    return "\n".join(formatted)

def duck_search(message: str) -> str:
    with DDGS() as ddgs:
        results = ddgs.text(message, max_results=7)
        snippets = "\n".join([r["body"] for r in results if "body" in r])
    return snippets or "Aucune information trouv√©e sur le web."