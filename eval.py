import random
import time
from typing import List
from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain.vectorstores import Chroma
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

CHROMA_DIR = "chroma_db"
EMBEDDING_MODEL = "nomic-embed-text"
LLM_MODEL = "llama3"  # ou mistral

TOP_K = 5  # Nombre de documents Ã  rÃ©cupÃ©rer
NUM_TESTS = 10  # Nombre dâ€™Ã©chantillons Ã  tester

# === GÃ©nÃ©rer des questions Ã  partir du contenu ===
def generate_questions_from_docs(docs: List[str]) -> List[tuple[str, str]]:
    llm = ChatOllama(model=LLM_MODEL)
    prompt = PromptTemplate.from_template(
        """Tu es un expert en transition Ã©cologique et en intelligence artificielle.
Ã€ partir de chaque contenu ci-dessous, gÃ©nÃ¨re une question pertinente liÃ©e au thÃ¨me de la **transition Ã©cologique** (changements climatiques, pollution, Ã©nergies renouvelables, recyclage, dÃ©veloppement durable, etc).

Ton objectif est de crÃ©er une question quâ€™une personne pourrait poser pour retrouver lâ€™information dans ce contenu.

Conserve la structure suivante :
(question ?, rÃ©sumÃ© du contenu)

Voici les contenus :
{docs}
"""
    )
    chain = LLMChain(llm=llm, prompt=prompt)

    combined_text = "\n\n".join([f"- {doc}" for doc in docs])
    response = chain.run({"docs": combined_text})

    # Supposons que le LLM retourne un texte structurÃ©
    qas = []
    for line in response.split("\n"):
        if "?" in line:
            parts = line.split("?", 1)
            question = parts[0].strip() + "?"
            if len(parts) > 1:
                answer = parts[1].strip(" .:-")
                qas.append((question, answer))
    return qas


# === Ã‰valuation de la prÃ©cision ===
def evaluate_precision():
    vectordb = Chroma(persist_directory=CHROMA_DIR, embedding_function=OllamaEmbeddings(model=EMBEDDING_MODEL))

    all_docs = vectordb.get(include=["documents", "metadatas"])
    docs = all_docs["documents"]
    ids = all_docs["ids"]

    print(f"ğŸ“„ Total de documents dans la base : {len(docs)}")

    sample = random.sample(list(zip(ids, docs)), min(NUM_TESTS, len(docs)))
    doc_texts = [doc for _, doc in sample]

    print("âœï¸ GÃ©nÃ©ration des questions avec le LLM...")
    qas = generate_questions_from_docs(doc_texts)

    correct = 0
    total = 0

    for i, ((question, expected_content), (expected_id, _)) in enumerate(zip(qas, sample), 1):
        print(f"\nğŸŸ¢ Question {i}: {question}")
        print(f"ğŸ”¹ Document attendu : {expected_id}")

        results = vectordb.similarity_search(question, k=TOP_K)
        retrieved_ids = [r.metadata.get("id") or r.metadata.get("source") or "unknown" for r in results]

        print(f"ğŸ” RÃ©sultats retournÃ©s: {retrieved_ids}")

        if expected_id in retrieved_ids:
            correct += 1
            print("âœ… Bon rÃ©sultat trouvÃ©.")
        else:
            print("âŒ Pas trouvÃ©.")

        total += 1
        time.sleep(1)  # Pour limiter les appels Ã  Ollama

    precision = correct / total if total else 0
    print(f"\nğŸ¯ PrÃ©cision@{TOP_K} : {precision:.2f} sur {total} questions.")


if __name__ == "__main__":
    evaluate_precision()