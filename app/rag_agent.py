from langchain_ollama import ChatOllama
from langchain import hub
from langchain_core.tools import Tool
from langchain.agents import AgentExecutor, create_react_agent
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from utils.search_chroma import documentSearch, duck_search
from utils.safe_memory import SafeConversationMemory

class RagAgent:
    """
    Classe RagAgent qui encapsule un agent ReAct (Reasoning + Acting) combinant
    recherche documentaire et recherche web pour r√©pondre aux questions.

    Cet agent utilise LangChain avec des outils (tools) configur√©s pour faire
    des recherches dans des documents internes et sur le web (via DuckDuckGo),
    en suivant un prompt strict imposant une m√©thode √©tape par √©tape.

    Attributs :
        model : Le mod√®le LLM utilis√© (ex: ChatOllama, ChatDeepSeek).
        system_prompt (str) : Le prompt syst√®me g√©n√©ral donn√© au mod√®le.
        memory : M√©moire conversationnelle s√©curis√©e pour stocker l'historique.
        tools (list) : Liste des outils (documentSearch et duck_search) pour les actions.
        prompt : Prompt sp√©cifique tir√© du hub LangChain (ex: "hwchase17/react").
        agent : Agent ReAct cr√©√© avec les outils et le mod√®le.
        executor : Ex√©cuteur pour g√©rer les interactions entre agent, m√©moire et outils.
    """

    def __init__(self, model, system_prompt: str, use_hub_prompt=True, verbose=True):
        """
        Initialise l'agent RagAgent avec le mod√®le, le prompt syst√®me et la configuration.

        Args:
            model : Mod√®le LLM √† utiliser pour l'agent.
            system_prompt (str) : Prompt syst√®me pour cadrer la conversation.
            use_hub_prompt (bool) : Si True, r√©cup√®re le prompt depuis LangChain Hub.
            verbose (bool) : Active les logs d√©taill√©s.
        
        Raises:
            ValueError : Si use_hub_prompt est False (non support√© ici).
        """
        self.model = model
        self.system_prompt = system_prompt

        # Initialise une m√©moire de conversation s√©curis√©e avec cl√©s pour l'entr√©e/sortie
        self.memory = SafeConversationMemory(
            memory_key="chat_history",
            return_messages=True,
            input_key="input",
            output_key="output"
        )

        # D√©finition des outils √† disposition de l'agent
        self.tools = [
            Tool(
                name="Recherche documents",
                func=documentSearch,
                description="Utilise les documents internes sur la transition √©cologique (lois, subventions, etc.)."
            ),
            Tool(
                name="Recherche web",
                func=duck_search,
                description="Utilise une recherche web pour des donn√©es √† jour sur la transition √©cologique."
            )
        ]

        # Chargement du prompt ReAct depuis le hub LangChain si demand√©
        if use_hub_prompt:
            self.prompt = hub.pull("hwchase17/react")
        else:
            raise ValueError("Mode 'use_hub_prompt=False' non pris en charge dans cette version")

        # Cr√©ation de l'agent ReAct avec le mod√®le, les outils et le prompt
        self.agent = create_react_agent(
            llm=self.model,
            tools=self.tools,
            prompt=self.prompt
        )

        # Cr√©ation de l'ex√©cuteur d'agent, avec m√©moire et gestion d'erreurs
        self.executor = AgentExecutor.from_agent_and_tools(
            agent=self.agent,
            tools=self.tools,
            memory=self.memory,
            verbose=verbose,
            handle_parsing_errors=True,
            max_iterations=7  # Limite du nombre d'it√©rations de r√©flexion/actes
        )

    def historique_to_prompt(self, historique):
        """
        Convertit l'historique des messages en une cha√Æne de texte format√©e.

        Args:
            historique (list): Liste des messages HumanMessage et AIMessage.

        Returns:
            str: Texte concat√©n√© avec pr√©fixes "Utilisateur :" et "Assistant :"
        """
        prompt = ""
        for message in historique:
            if isinstance(message, HumanMessage):
                prompt += f"Utilisateur : {message.content}\n"
            elif isinstance(message, AIMessage):
                prompt += f"Assistant : {message.content}\n"
        return prompt.strip()

    def search(self, historique):
        """
        Lance une recherche et interaction avec l'agent ReAct √† partir de l'historique.

        Cette m√©thode construit un prompt complet incluant un "injection" avec les r√®gles
        strictes √† suivre, puis ex√©cute l'agent avec l'historique donn√©, et filtre la sortie
        pour extraire la r√©ponse finale et les sources.

        Args:
            historique (list): Liste des messages pr√©c√©dents (HumanMessage, AIMessage).

        Returns:
            str: R√©ponse finale filtr√©e contenant la r√©ponse et la source.
        """
        # Combine le prompt syst√®me et l'historique en messages
        messages = [SystemMessage(content=self.system_prompt)] + historique

        # Injection du prompt ReAct strict d√©taillant les r√®gles √† suivre
        injection = (
        "Tu es un agent ReAct. Tu dois OBLIGATOIREMENT suivre ce format exact √† chaque √©tape :\n\n"
        "Question: <question>\n"
        "Thought: <r√©flexion sur la prochaine √©tape>\n"
        "Action: <choisir uniquement [Recherche documents] ou [Recherche web]>\n"
        "Action Input: <requ√™te √† rechercher>\n"
        "Observation: <r√©sultat obtenu>\n\n"
        "tu termines par :\n"
        "Thought: J'ai r√©uni suffisamment d'informations.\n"
        "Final Answer: <r√©ponse finale claire et concise en fran√ßais>\n"
        "Source : <Documents, Web, IA ou combinaison>\n\n"
        "‚ö†Ô∏è Tu DOIS commencer par une [Recherche documents]. C'est OBLIGATOIRE.\n"
        "‚ö†Ô∏è Tu DOIS int√©grer les documents trouv√©s dans ta r√©ponse, m√™me s‚Äôils ne suffisent pas.\n"
        "‚ö†Ô∏è Tu NE PEUX PAS r√©pondre avec l‚ÄôIA seule, sauf si documents ET web √©chouent compl√®tement.\n"
        "‚ö†Ô∏è Le web est un dernier recours, jamais le premier.\n"
        "NE DONNE AUCUNE r√©ponse sans source explicite. NE SAUTE AUCUNE √âTAPE.\n\n"
        "Exemple :\n"
        "Question: Quelle est l‚Äôempreinte carbone totale de la France en 2021 ?\n"
        "Thought: Je commence par chercher dans les documents.\n"
        "Action: Recherche documents\n"
        "Action Input: empreinte carbone France 2021\n"
        "Observation: Les documents indiquent environ 663 millions de tonnes √©quivalent CO2.\n"
        "Thought: J'ai r√©uni suffisamment d'informations.\n"
        "Final Answer: L'empreinte carbone totale de la France en 2021 √©tait d'environ 663 millions de tonnes √©quivalent CO2.\n"
        "Source : Documents\n"
        )

        # Construction finale du prompt envoy√© √† l'agent
        prompt_text = injection + "\n\n" + self.historique_to_prompt(historique)

        print("\nüü¶ Prompt envoy√© √† l‚Äôagent :\n", prompt_text)

         # Invocation de l'agent avec le prompt et l'historique de conversation
        response = self.executor.invoke({
            "input": prompt_text,
            "chat_history": messages
        })

        # Extraction du texte de sortie brut
        output = response.get("output", "") if isinstance(response, dict) else str(response)

        def filter_output(text: str) -> str:
            """
            Filtre la sortie brute pour extraire la r√©ponse finale et la source.

            Args:
                text (str): Texte brut g√©n√©r√© par l'agent.

            Returns:
                str: R√©ponse finale format√©e avec la source si pr√©sente.
            """
            lines = text.splitlines()
            final_answer = None
            source = None
            for line in lines:
                lline = line.lower().strip()
                if lline.startswith("final answer:"):
                    final_answer = line.split(":", 1)[1].strip()
                elif lline.startswith("source :"):
                    source = line.split(":", 1)[1].strip()
            if final_answer is None:
                return text.strip()
            if source:
                return f"{final_answer}\n\nSource : {source}"
            return final_answer
        
        # Application du filtre sur la sortie brute
        final_output = filter_output(output)

        print("\nüü© R√©sultat filtr√© :\n", final_output)
        return final_output
