from langchain_ollama import ChatOllama
from langchain import hub
from langchain_core.tools import Tool
from langchain.agents import AgentExecutor, create_react_agent
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from utils.search_chroma import documentSearch, duck_search
from utils.safe_memory import SafeConversationMemory

class RagAgent:
    def __init__(self, model, system_prompt: str, use_hub_prompt=True, verbose=True):
        self.model = model
        self.system_prompt = system_prompt

        self.memory = SafeConversationMemory(
            memory_key="chat_history",
            return_messages=True,
            input_key="input",
            output_key="output"
        )

        self.tools = [
            Tool(
                name="Recherche documents",
                func=documentSearch,
                description="Utilise les documents internes sur la transition √©cologique (lois, subventions, etc.)."
            ),
            Tool(
                name="Recherche web",
                func=duck_search,
                description="Utilise une recherche web pour des donn√©es √† jour sur la transition √©cologique."
            )
        ]

        if use_hub_prompt:
            self.prompt = hub.pull("hwchase17/react")
        else:
            raise ValueError("Mode 'use_hub_prompt=False' non pris en charge dans cette version")

        self.agent = create_react_agent(
            llm=self.model,
            tools=self.tools,
            prompt=self.prompt
        )

        self.executor = AgentExecutor.from_agent_and_tools(
            agent=self.agent,
            tools=self.tools,
            memory=self.memory,
            verbose=verbose,
            handle_parsing_errors=True,
            max_iterations=7
        )

    def historique_to_prompt(self, historique):
        prompt = ""
        for message in historique:
            if isinstance(message, HumanMessage):
                prompt += f"Utilisateur : {message.content}\n"
            elif isinstance(message, AIMessage):
                prompt += f"Assistant : {message.content}\n"
        return prompt.strip()

    def search(self, historique):
        messages = [SystemMessage(content=self.system_prompt)] + historique

        injection = (
        "Tu es un agent ReAct. Tu dois OBLIGATOIREMENT suivre ce format exact √† chaque √©tape :\n\n"
        "Question: <question>\n"
        "Thought: <r√©flexion sur la prochaine √©tape>\n"
        "Action: <choisir uniquement [Recherche documents] ou [Recherche web]>\n"
        "Action Input: <requ√™te √† rechercher>\n"
        "Observation: <r√©sultat obtenu>\n\n"
        "tu termines par :\n"
        "Thought: J'ai r√©uni suffisamment d'informations.\n"
        "Final Answer: <r√©ponse finale claire et concise en fran√ßais>\n"
        "Source : <Documents, Web, IA ou combinaison>\n\n"
        "‚ö†Ô∏è Tu DOIS commencer par une [Recherche documents]. C'est OBLIGATOIRE.\n"
        "‚ö†Ô∏è Tu DOIS int√©grer les documents trouv√©s dans ta r√©ponse, m√™me s‚Äôils ne suffisent pas.\n"
        "‚ö†Ô∏è Tu NE PEUX PAS r√©pondre avec l‚ÄôIA seule, sauf si documents ET web √©chouent compl√®tement.\n"
        "‚ö†Ô∏è Le web est un dernier recours, jamais le premier.\n"
        "NE DONNE AUCUNE r√©ponse sans source explicite. NE SAUTE AUCUNE √âTAPE.\n\n"
        "Exemple :\n"
        "Question: Quelle est l‚Äôempreinte carbone totale de la France en 2021 ?\n"
        "Thought: Je commence par chercher dans les documents.\n"
        "Action: Recherche documents\n"
        "Action Input: empreinte carbone France 2021\n"
        "Observation: Les documents indiquent environ 663 millions de tonnes √©quivalent CO2.\n"
        "Thought: J'ai r√©uni suffisamment d'informations.\n"
        "Final Answer: L'empreinte carbone totale de la France en 2021 √©tait d'environ 663 millions de tonnes √©quivalent CO2.\n"
        "Source : Documents\n"
        )


        prompt_text = injection + "\n\n" + self.historique_to_prompt(historique)

        print("\nüü¶ Prompt envoy√© √† l‚Äôagent :\n", prompt_text)

        # Ex√©cution de l'agent
        response = self.executor.invoke({
            "input": prompt_text,
            "chat_history": messages
        })

        # Extraction du texte brut
        output = response.get("output", "") if isinstance(response, dict) else str(response)

        def filter_output(text: str) -> str:
            lines = text.splitlines()
            final_answer = None
            source = None
            for line in lines:
                lline = line.lower().strip()
                if lline.startswith("final answer:"):
                    final_answer = line.split(":", 1)[1].strip()
                elif lline.startswith("source :"):
                    source = line.split(":", 1)[1].strip()
            if final_answer is None:
                return text.strip()
            if source:
                return f"{final_answer}\n\nSource : {source}"
            return final_answer

        final_output = filter_output(output)

        print("\nüü© R√©sultat filtr√© :\n", final_output)
        return final_output
