import sys
import os

# Ajouter le dossier racine √† sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from IPython.display import display, clear_output, Markdown
from dotenv import load_dotenv
from datetime import datetime
from langchain_ollama import ChatOllama
#from langchain_deepseek import ChatDeepSeek
from langchain import hub
from langchain_core.tools import Tool
from langchain.agents import AgentExecutor, create_react_agent
from langchain.memory import ConversationBufferMemory
import re
from langchain.prompts import PromptTemplate
from langchain_core.messages import HumanMessage, AIMessage
#from LLM.utils.rag.document import document_search
#from LLM.utils.rag.web_search import web_search

from utils.search_chroma import *
from duck_search import duck_search


SPECIALIZED_INSTRUCTION = """
Tu es un assistant intelligent expert en **transition √©cologique**. Ton objectif est d‚Äôaider les citoyens √† comprendre :
- les lois et r√©glementations,
- les subventions ou aides disponibles,
- les bonnes pratiques pour la transition √©nerg√©tique et environnementale.
- Tu t‚Äôexprimes **en fran√ßais clair**, sans emoji, ni fioritures.

Tu as acc√®s √† deux outils :
1. **Recherche documents** : pour chercher dans les documents internes fiables (ex. l√©gislation, politiques publiques, aides existantes).
2. **Recherche web** : pour compl√©ter avec des donn√©es √† jour disponibles sur Internet si les documents internes ne suffisent pas.

---

Lorsque tu r√©ponds :
- Utilise **d'abord** l‚Äôoutil `Recherche documents`.
- Si l‚Äôinformation n‚Äôest **pas disponible ou insuffisante**, utilise `Recherche web`.
- Tu **dois toujours suivre la structure ReAct** :

Exemple de raisonnement attendu :
Thought: Je dois chercher la r√©glementation actuelle sur les panneaux solaires.
Action: Recherche documents
Action Input: r√©glementation panneaux solaires France 2024


Puis, apr√®s avoir re√ßu l'information :

Observation: Voici les r√©sultats des documents internes...
Pens√©e: Les documents internes r√©pondent bien √† la question.
R√©ponse: En 2024, les r√®gles pour les panneaux solaires sont...


---

Tu ne dois **jamais inventer une r√©ponse**.
Si aucune information fiable n‚Äôest disponible apr√®s les recherches, r√©ponds simplement : `Je ne sais pas.`

Tu t‚Äôexprimes **en fran√ßais clair**, sans emoji, ni fioritures.

---

Voici la question :

"""

class RagAgent:
    def __init__(self, model, use_hub_prompt=True):
        self.model = model

        self.tools = [
            Tool(
                name="Recherche documents",
                func=documentSearch,
                description="Utilise les documents internes sur la transition √©cologique (lois, subventions, etc.)."
            ),
            Tool(
                name="Recherche web",
                func=duck_search,
                description="Utilise une recherche web pour des donn√©es √† jour sur la transition √©cologique."
            ),
        ]

        if use_hub_prompt:
            base_prompt = hub.pull("hwchase17/react")  # ReAct de base
            # Fusionne instruction sp√©cialis√©e et prompt de base
            base_prompt.template = f"{SPECIALIZED_INSTRUCTION}\n\n{base_prompt.template}"
            self.prompt = base_prompt
        else:
            raise ValueError("Mode 'use_hub_prompt=False' non pris en charge dans cette version")

        self.agent = create_react_agent(
            llm=self.model,
            tools=self.tools,
            prompt=self.prompt
        )

        self.executor = AgentExecutor.from_agent_and_tools(
            agent=self.agent,
            tools=self.tools,
            verbose=True,
            handle_parsing_errors=True
        )

    def historique_to_prompt(self, historique):
        prompt = ""
        for message in historique:
            if isinstance(message, HumanMessage):
                prompt += f"Utilisateur : {message.content}\n"
            elif isinstance(message, AIMessage):
                prompt += f"Assistant : {message.content}\n"
        return prompt.strip()

    def search(self, historique):
        prompt = self.historique_to_prompt(historique)
        return self.executor.invoke({"input": prompt})

    def boucle_interactive(self):
        historique = []
        print("üü¢ Assistant transition √©cologique (entrez 'exit' pour quitter)\n")
        while True:
            user_input = input("Vous : ")
            if user_input.strip().lower() in ["exit", "quit", "stop"]:
                print("üëã Fin de la session.")
                break
            historique.append(HumanMessage(content=user_input))
            clear_output(wait=True)
            display(Markdown(f"**Vous :** {user_input}"))

            response = self.search(historique)
            if "output" in response:
                answer = response["output"]
                historique.append(AIMessage(content=answer))
                display(Markdown(f"**Assistant :** {answer}"))
            else:
                display(Markdown("**Erreur dans la r√©ponse.**"))

    
if __name__ == "__main__":
    agent = RagAgent(ChatOllama(model="llama3"))
    agent.boucle_interactive()

