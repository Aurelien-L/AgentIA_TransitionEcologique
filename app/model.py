import os
from dotenv import load_dotenv
from langchain_deepseek import ChatDeepSeek
from langchain_ollama import ChatOllama
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from .rag_agent import RagAgent

USE_DEEPSEEK = True  # â¬…ï¸ Mets sur False pour revenir Ã  Llama3

# Ceci permet d'utiliser des modÃ¨les en ligne comme gpt-x, deepseek-x, etc...
load_dotenv(override=True) 
# ğŸ” Choix du modÃ¨le

if load_dotenv(override=True) and USE_DEEPSEEK:
    MODEL_NAME = "deepseek-chat"
    llm = ChatDeepSeek(model=MODEL_NAME, api_key=os.getenv("DEEPSEEK_API_KEY"))
else:
    MODEL_NAME = "llama3"
    llm = ChatOllama(model=MODEL_NAME, temperature=0)

SYSTEM_PROMPT = """
Tu es un assistant intelligent spÃ©cialisÃ© dans la transition Ã©cologique et les services publics. Tu aides les citoyens Ã  comprendre des informations administratives, environnementales ou techniques de maniÃ¨re claire, utile et bienveillante.

Tu disposes de plusieurs outils, dont :
- Recherche documents : pour interroger des documents internes fiables et validÃ©s (rapports, textes rÃ©glementaires, Ã©tudes publiques),
- Recherche web : pour chercher des informations complÃ©mentaires sur internet, en dernier recours.

âš ï¸ Tu dois TOUJOURS commencer par lâ€™outil **Recherche documents**, sauf si l'utilisateur demande explicitement une recherche sur Internet ou un site externe. 
âš ï¸ Tu nâ€™utilises lâ€™outil Recherche web **que si la recherche documentaire ne donne pas de rÃ©ponse satisfaisante et une l'utilise que trois quatre fois maximum.**
âš ï¸ Tu ne propose pas d'option supplÃ©mentaire ni de choix **tu ne fournis qu'une rÃ©ponse claire et net et si tu n'as pas de rÃ©ponse tu renvoie un je ne sais pas**.

Tu suis **scrupuleusement** le format ReAct suivant :

PensÃ©e : (ta rÃ©flexion pour comprendre la demande et dÃ©cider de lâ€™action)
Action : (le nom exact dâ€™un outil Ã  utiliser, sans faute)
EntrÃ©e de lâ€™action : (le texte Ã  transmettre Ã  lâ€™outil)
Observation : (le retour de lâ€™outil)
PensÃ©e : (ce que tu en conclus)
RÃ©ponse : (ta rÃ©ponse finale, claire, utile, contextualisÃ©e)

âš ï¸ Tu tâ€™exprimes TOUJOURS EN FRANÃ‡AIS, mÃªme si les documents ou les outils sont en anglais.

Ta rÃ©ponse doit Ãªtre :
- claire et bienveillante,
- adaptÃ©e Ã  un large public (y compris non expert ou non francophone),
- et reflÃ©ter un souci de pÃ©dagogie et de rigueur.

Si lâ€™information est incertaine, partielle ou absente :
- explique-le avec honnÃªtetÃ©,
- propose une reformulation de la demande, ou
- suggÃ¨re des pistes ou sources fiables Ã  consulter.

Ne prÃ©tends jamais disposer de donnÃ©es en temps rÃ©el si ce nâ€™est pas le cas.

Ton but est de rendre service de maniÃ¨re fiable, en aidant Ã  comprendre et agir pour la transition Ã©cologique et les services publics.

"""

RESPONSE_MARKERS = [
    "rÃ©ponse :", 
    "final answer", 
    "voici ce que je recommande", 
    "donc", 
    "en rÃ©sumÃ©"
]

class ChatModel:
    def __init__(self, model=llm, system_prompt=SYSTEM_PROMPT):
        self.system_prompt = system_prompt
        self.llm = model
        self.historique = [SystemMessage(content=system_prompt)]
        self.agent_rag = RagAgent(self.llm, system_prompt=self.system_prompt)

    def model_response(self, message: str) -> str:
        self.historique.append(HumanMessage(content=message))

        try:
            response = self.llm.invoke(self.historique)
            output = response.content.strip()
        except Exception as e:
            print(f"[âš ï¸ Erreur modÃ¨le principal] {e}")
            output = ""

        # ğŸ” VÃ©rifie si la rÃ©ponse est insuffisante
        is_short = len(output) < 20
        is_generic = output.lower() in ["je ne sais pas.", "je ne suis pas sÃ»r."]
        lacks_final = not any(marker in output.lower() for marker in RESPONSE_MARKERS)

        if not output or is_short or is_generic or lacks_final:
            print("ğŸ” Passage au RAG agent pour une meilleure rÃ©ponse")
            rag_output = self.agent_rag.search(self.historique)
            final_response = rag_output.get("output") if isinstance(rag_output, dict) else str(rag_output)
        else:
            final_response = output

        self.historique.append(AIMessage(content=final_response))
        return final_response

if __name__ == "__main__":
    # Instanciation directe pour tester l'agent
    print(f"ğŸ”§ ModÃ¨le utilisÃ© : {MODEL_NAME}")
    model = ChatOllama(model=MODEL_NAME)
    from rag_agent import RagAgent  # Import ici pour Ã©viter le cycle Ã  l'import

    agent = RagAgent(model=llm, system_prompt=SYSTEM_PROMPT)
    agent.boucle_interactive()