import os
from dotenv import load_dotenv
from langchain_deepseek import ChatDeepSeek
from langchain_ollama import ChatOllama
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from .rag_agent import RagAgent

USE_DEEPSEEK = True  # ‚¨ÖÔ∏è Mets sur False pour revenir √† Llama3

# Chargement des variables d'environnement depuis un fichier .env
load_dotenv(override=True) 

# üîÅ Choix du mod√®le √† utiliser selon la variable USE_DEEPSEEK et la pr√©sence des cl√©s d'API
if load_dotenv(override=True) and USE_DEEPSEEK:
    MODEL_NAME = "deepseek-chat"  # Nom du mod√®le DeepSeek √† utiliser
    # Initialisation de l'instance LLM DeepSeek avec cl√© API r√©cup√©r√©e dans les variables d'environnement
    llm = ChatDeepSeek(model=MODEL_NAME, api_key=os.getenv("DEEPSEEK_API_KEY"))
else:
    MODEL_NAME = "llama3"  # Sinon on revient √† Llama3
    # Initialisation du mod√®le Llama3 avec temp√©rature 0 (r√©ponses d√©terministes)
    llm = ChatOllama(model=MODEL_NAME, temperature=0)

# PROMPT SYST√àME utilis√© pour guider le comportement de l'assistant intelligent
SYSTEM_PROMPT = """
Tu es un assistant intelligent sp√©cialis√© dans les questions li√©es √† la transition √©cologique.

Tu suis la m√©thode ReAct (Reasoning + Acting) avec les r√®gles suivantes :

1. Tu DOIS toujours commencer par une **Recherche documents**.
2. Tu dois OBLIGATOIREMENT inclure les r√©sultats de la recherche documentaire dans ta r√©ponse finale, m√™me partiellement.
3. Tu ne peux effectuer une **Recherche web** que si les documents ne suffisent pas, et tu dois le justifier dans ta r√©flexion.
4. Tu ne peux faire de **raisonnement IA** (sans source) qu‚Äôen tout dernier recours absolu, et uniquement si les documents ET le web sont vides ou non pertinents.
5. Ta r√©ponse finale doit √™tre fond√©e sur des sources et contenir obligatoirement la mention :  
   **Source : Documents, Web, IA ou combinaison**

**Format strict √† respecter √† chaque √©tape :**

Question: <question de l'utilisateur>  
Thought: <ta r√©flexion sur la prochaine √©tape>  
Action: <choisis uniquement "Recherche documents" ou "Recherche web">  
Action Input: <requ√™te √† rechercher>  
Observation: <r√©sultat de la recherche>  

tu termines par :  
Thought: J'ai r√©uni suffisamment d'informations.  
Final Answer: <r√©ponse finale claire et concise, en fran√ßais>  
Source : <indique la ou les sources utilis√©es : Documents, Web, IA ou combinaison>

---

Exemple :

Question: Quelle est l‚Äôempreinte carbone totale de la France en 2021 ?  
Thought: Je commence par chercher dans les documents officiels.  
Action: Recherche documents  
Action Input: empreinte carbone France 2021  
Observation: Les documents indiquent que l‚Äôempreinte carbone totale √©tait d‚Äôenviron 663 millions de tonnes √©quivalent CO2.  
Thought: J'ai r√©uni suffisamment d'informations.  
Final Answer: L'empreinte carbone totale de la France en 2021 √©tait d'environ 663 millions de tonnes √©quivalent CO2.  
Source : Documents

---

‚ö†Ô∏è NE JAMAIS donner de r√©ponse IA sans avoir exploit√© les documents.  
‚ö†Ô∏è Le Web est un compl√©ment optionnel si les documents sont insuffisants.  
‚ö†Ô∏è Ne saute aucune √©tape, ne change jamais le format, respecte strictement la structure.
"""


# Liste des mots cl√©s pour d√©tecter une r√©ponse finale dans la sortie du mod√®le
RESPONSE_MARKERS = ["r√©ponse", "final answer", "source :"]

class ChatModel:
    """
    Classe repr√©sentant le mod√®le de chat intelligent combinant un LLM (DeepSeek ou Llama3)
    avec un agent RAG (Recherche Augment√©e par G√©n√©ration) pour g√©rer la logique ReAct.
    """

    def __init__(self, model=llm, system_prompt=SYSTEM_PROMPT):
        """
        Initialise le mod√®le de chat avec un mod√®le LLM et un prompt syst√®me.

        Args:
            model: instance du mod√®le LLM (par d√©faut celui choisi plus haut)
            system_prompt: cha√Æne de caract√®res d√©finissant le prompt syst√®me pour guider l'agent
        """
        self.system_prompt = system_prompt
        self.llm = model
        # Historique des messages √©chang√©s (avec un message syst√®me initial)
        self.historique = [SystemMessage(content=system_prompt)]
        # Initialisation de l'agent RAG avec le m√™me LLM et prompt
        self.agent_rag = RagAgent(self.llm, system_prompt=system_prompt)

    def _filter_final_answer_and_source(self, text: str) -> str:
        """
        Extrait la r√©ponse finale et la source dans le texte renvoy√© par l'agent,
        en respectant le format attendu (final answer + source).

        Args:
            text: cha√Æne de caract√®res contenant la sortie brute du mod√®le

        Returns:
            Une cha√Æne avec uniquement la r√©ponse finale et la source format√©e,
            ou le texte original si aucun marqueur n'a √©t√© trouv√©.
        """
        final_answer = None
        source = None

        # Analyse ligne par ligne pour trouver "final answer" et "source"
        for line in text.splitlines():
            line_lower = line.lower().strip()
            if line_lower.startswith("final answer:"):
                final_answer = line.split(":", 1)[1].strip()
            elif line_lower.startswith("source :"):
                source = line.split(":", 1)[1].strip()

        # Si aucune r√©ponse finale, on retourne le texte brut (ex : message d'erreur)
        if final_answer is None:
            return text.strip()

        # Si la source est pr√©sente, on la concat√®ne proprement √† la r√©ponse finale
        if source:
            return f"{final_answer}\n\nSource : {source}"
        else:
            return final_answer

    def model_response(self, message: str) -> str:
        """
        Traite un message utilisateur, interroge l'agent RAG, g√®re les exceptions,
        filtre la r√©ponse pour ne garder que la r√©ponse finale et la source,
        et met √† jour l'historique de la conversation.

        Args:
            message: message texte de l'utilisateur

        Returns:
            La r√©ponse finale format√©e √† retourner √† l'utilisateur.
        """
        # Ajout du message utilisateur √† l'historique
        self.historique.append(HumanMessage(content=message))

        try:
            # Recherche via l'agent RAG avec l'historique complet
            rag_response = self.agent_rag.search(self.historique)

            # Gestion tol√©rante selon que la r√©ponse est dict ou str
            if isinstance(rag_response, dict) and "output" in rag_response:
                output = rag_response["output"].strip()
            elif isinstance(rag_response, str):
                output = rag_response.strip()
            else:
                output = ""

        except Exception as e:
            # En cas d'erreur dans RagAgent, on affiche un avertissement et continue
            print(f"[‚ö†Ô∏è Erreur RagAgent] {e}")
            output = ""

        # Si la sortie est trop courte ou ne contient pas les mots cl√©s attendus,
        # on appelle directement le LLM en fallback
        if len(output) < 20 or not any(m in output.lower() for m in RESPONSE_MARKERS):
            try:
                output = self.llm.invoke(self.historique).content.strip()
            except Exception as e:
                # En cas d'erreur LLM direct, on retourne une r√©ponse g√©n√©rique
                print(f"[‚ö†Ô∏è Erreur LLM direct] {e}")
                output = "Je ne sais pas."

        # On filtre la sortie pour garder uniquement la r√©ponse finale et la source
        filtered_output = self._filter_final_answer_and_source(output)

        # On ajoute la r√©ponse AI √† l'historique pour conserver le contexte
        self.historique.append(AIMessage(content=filtered_output))

        # Retour de la r√©ponse finale filtr√©e
        return filtered_output

