import os
from dotenv import load_dotenv
from langchain_deepseek import ChatDeepSeek
from langchain_ollama import ChatOllama
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from .rag_agent import RagAgent

USE_DEEPSEEK = True  # ‚¨ÖÔ∏è Mets sur False pour revenir √† Llama3

# Ceci permet d'utiliser des mod√®les en ligne comme gpt-x, deepseek-x, etc...
load_dotenv(override=True) 
# üîÅ Choix du mod√®le

if load_dotenv(override=True) and USE_DEEPSEEK:
    MODEL_NAME = "deepseek-chat"
    llm = ChatDeepSeek(model=MODEL_NAME, api_key=os.getenv("DEEPSEEK_API_KEY"))
else:
    MODEL_NAME = "llama3"
    llm = ChatOllama(model=MODEL_NAME, temperature=0)

SYSTEM_PROMPT = """
Vous √™tes un assistant intelligent utilisant la m√©thode ReAct (Reasoning + Acting).

Pour r√©pondre √† la question de l'utilisateur, vous devez suivre ce format pr√©cis et strict, √©tape par √©tape :

Question: <question pos√©e par l'utilisateur>

Thought: <votre r√©flexion sur la prochaine √©tape √† effectuer>

Action: <choisissez exactement une action parmi : "Recherche documents" ou "Recherche web">

Action Input: <la requ√™te exacte que vous envoyez √† cette action>

Observation: <les r√©sultats ou informations obtenues suite √† l'action>

Vous pouvez r√©p√©ter autant de fois que n√©cessaire la s√©quence Thought ‚Üí Action ‚Üí Action Input ‚Üí Observation.

Quand vous disposez d'assez d'informations pour r√©pondre, terminez par :

Thought: J'ai r√©uni suffisamment d'informations.

Final Answer: <votre r√©ponse compl√®te et concise en fran√ßais>

Important :  
- Apr√®s chaque Thought, vous devez obligatoirement sp√©cifier une Action.  
- L'Action doit √™tre exactement "Recherche documents" ou "Recherche web".  
- Respectez strictement la casse, la ponctuation, les espaces et le format indiqu√©.  
- Ne donnez aucune explication ou commentaire hors du cadre indiqu√©.  
- La r√©ponse finale doit √™tre factuelle, claire et concise.
- Termine toujours la r√©ponse finale par : **Source : <...>** (Documents, Web, IA ou une combinaison).
"""

RESPONSE_MARKERS = [
    "r√©ponse :", 
    "final answer", 
    "voici ce que je recommande", 
    "donc", 
    "en r√©sum√©",
    "source :"
]

class ChatModel:
    def __init__(self, model=llm, system_prompt=SYSTEM_PROMPT):
        self.system_prompt = system_prompt
        self.llm = model
        self.historique = [SystemMessage(content=system_prompt)]
        self.agent_rag = RagAgent(self.llm, system_prompt=self.system_prompt)

    def model_response(self, message: str) -> str:
        self.historique.append(HumanMessage(content=message))

        try:
            rag_response = self.agent_rag.search(self.historique)
            if isinstance(rag_response, dict):
                output = rag_response.get("output", "").strip()
            else:
                output = str(rag_response).strip()
        except Exception as e:
            print(f"[‚ö†Ô∏è Erreur RagAgent] {e}")
            output = ""

        is_short = len(output) < 20
        is_generic = output.lower() in ["je ne sais pas.", "je ne suis pas s√ªr.", ""]
        lacks_final = not any(marker in output.lower() for marker in RESPONSE_MARKERS)
        missing_source_tag = not "source :" in output.lower()

        if is_short or is_generic or lacks_final or missing_source_tag:
            try:
                response = self.llm.invoke(self.historique)
                output = response.content.strip()
            except Exception as e:
                print(f"[‚ö†Ô∏è Erreur mod√®le principal] {e}")
                output = "Je ne sais pas."

        self.historique.append(AIMessage(content=output))
        return output
