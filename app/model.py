import os
from dotenv import load_dotenv
from langchain_deepseek import ChatDeepSeek
from langchain_ollama import ChatOllama
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from .rag_agent import RagAgent

USE_DEEPSEEK = True  # ‚¨ÖÔ∏è Mets sur False pour revenir √† Llama3

# Ceci permet d'utiliser des mod√®les en ligne comme gpt-x, deepseek-x, etc...
load_dotenv(override=True) 
# üîÅ Choix du mod√®le

if load_dotenv(override=True) and USE_DEEPSEEK:
    MODEL_NAME = "deepseek-chat"
    llm = ChatDeepSeek(model=MODEL_NAME, api_key=os.getenv("DEEPSEEK_API_KEY"))
else:
    MODEL_NAME = "llama3"
    llm = ChatOllama(model=MODEL_NAME, temperature=0)

SYSTEM_PROMPT = """
Tu es un assistant intelligent sp√©cialis√© dans les questions li√©es √† la transition √©cologique.

Tu suis la m√©thode ReAct (Reasoning + Acting) avec les r√®gles suivantes :

1. Tu DOIS toujours commencer par une **Recherche documents**.
2. Tu dois OBLIGATOIREMENT inclure les r√©sultats de la recherche documentaire dans ta r√©ponse finale, m√™me partiellement.
3. Tu ne peux effectuer une **Recherche web** que si les documents ne suffisent pas, et tu dois le justifier dans ta r√©flexion.
4. Tu ne peux faire de **raisonnement IA** (sans source) qu‚Äôen tout dernier recours absolu, et uniquement si les documents ET le web sont vides ou non pertinents.
5. Ta r√©ponse finale doit √™tre fond√©e sur des sources et contenir obligatoirement la mention :  
   **Source : Documents, Web, IA ou combinaison**

**Format strict √† respecter √† chaque √©tape :**

Question: <question de l'utilisateur>  
Thought: <ta r√©flexion sur la prochaine √©tape>  
Action: <choisis uniquement "Recherche documents" ou "Recherche web">  
Action Input: <requ√™te √† rechercher>  
Observation: <r√©sultat de la recherche>  

tu termines par :  
Thought: J'ai r√©uni suffisamment d'informations.  
Final Answer: <r√©ponse finale claire et concise, en fran√ßais>  
Source : <indique la ou les sources utilis√©es : Documents, Web, IA ou combinaison>

---

Exemple :

Question: Quelle est l‚Äôempreinte carbone totale de la France en 2021 ?  
Thought: Je commence par chercher dans les documents officiels.  
Action: Recherche documents  
Action Input: empreinte carbone France 2021  
Observation: Les documents indiquent que l‚Äôempreinte carbone totale √©tait d‚Äôenviron 663 millions de tonnes √©quivalent CO2.  
Thought: J'ai r√©uni suffisamment d'informations.  
Final Answer: L'empreinte carbone totale de la France en 2021 √©tait d'environ 663 millions de tonnes √©quivalent CO2.  
Source : Documents

---

‚ö†Ô∏è NE JAMAIS donner de r√©ponse IA sans avoir exploit√© les documents.  
‚ö†Ô∏è Le Web est un compl√©ment optionnel si les documents sont insuffisants.  
‚ö†Ô∏è Ne saute aucune √©tape, ne change jamais le format, respecte strictement la structure.
"""


RESPONSE_MARKERS = ["r√©ponse", "final answer", "source :"]

class ChatModel:
    def __init__(self, model=llm, system_prompt=SYSTEM_PROMPT):
        self.system_prompt = system_prompt
        self.llm = model
        self.historique = [SystemMessage(content=system_prompt)]
        self.agent_rag = RagAgent(self.llm, system_prompt=system_prompt)

    def _filter_final_answer_and_source(self, text: str) -> str:
        final_answer = None
        source = None

        for line in text.splitlines():
            line_lower = line.lower().strip()
            if line_lower.startswith("final answer:"):
                final_answer = line.split(":", 1)[1].strip()
            elif line_lower.startswith("source :"):
                source = line.split(":", 1)[1].strip()

        if final_answer is None:
            return text.strip()

        if source:
            return f"{final_answer}\n\nSource : {source}"
        else:
            return final_answer

    def model_response(self, message: str) -> str:
        self.historique.append(HumanMessage(content=message))

        try:
            rag_response = self.agent_rag.search(self.historique)

            # ‚úÖ Nouvelle version tol√©rante : dict ou str
            if isinstance(rag_response, dict) and "output" in rag_response:
                output = rag_response["output"].strip()
            elif isinstance(rag_response, str):
                output = rag_response.strip()
            else:
                output = ""

        except Exception as e:
            print(f"[‚ö†Ô∏è Erreur RagAgent] {e}")
            output = ""

        if len(output) < 20 or not any(m in output.lower() for m in RESPONSE_MARKERS):
            try:
                output = self.llm.invoke(self.historique).content.strip()
            except Exception as e:
                print(f"[‚ö†Ô∏è Erreur LLM direct] {e}")
                output = "Je ne sais pas."

        filtered_output = self._filter_final_answer_and_source(output)

        self.historique.append(AIMessage(content=filtered_output))
        return filtered_output

