# ğŸ“š Indexation de Documents StructurÃ©s avec Chroma & LangChain

## ğŸ§  Objectif du code
Ce code vise Ã  automatiser le nettoyage, le dÃ©coupage et lâ€™indexation de fichiers structurÃ©s (au format Parquet) dans une base vectorielle persistante (Chroma). Lâ€™objectif est de permettre une recherche sÃ©mantique efficace via des embeddings gÃ©nÃ©rÃ©s Ã  lâ€™aide du modÃ¨le nomic-embed-text.

## ğŸ”§ Pourquoi ce code a Ã©tÃ© conÃ§u

**ProblÃ¨me rencontrÃ© :**
- Les fichiers de donnÃ©es changent rÃ©guliÃ¨rement (ajouts, corrections, etc.).
- Recalculer tous les embeddings Ã  chaque changement serait inefficace et coÃ»teux.
- Il fallait une solution incrÃ©mentale et robuste pour :
- Nettoyer les donnÃ©es,
- GÃ©nÃ©rer uniquement les embeddings des nouvelles donnÃ©es,
- Mettre Ã  jour la base vectorielle de maniÃ¨re fiable.

**Solution mise en place :**
- Cache local (`index_cache.json`) pour suivre les fichiers dÃ©jÃ  indexÃ©s via un hash MD5.
- DÃ©tection automatique des fichiers modifiÃ©s ou nouveaux.
- DÃ©coupage intelligent des textes avec RecursiveCharacterTextSplitter.
- DÃ©duplication par hash de contenu.
- Indexation incrÃ©mentale dans Chroma, avec gestion des IDs uniques.

## ğŸ› ï¸ FonctionnalitÃ©s principales
- âœ… Nettoyage des donnÃ©es via `clean_all()` (modularisÃ© dans utils/chroma/run_cleaning.py).

- ğŸ“¦ Chargement dynamique des fichiers `.parquet` dans data/clean/.

- âœ‚ï¸ DÃ©coupage intelligent en chunks de texte de taille contrÃ´lÃ©e.

- ğŸ§  GÃ©nÃ©ration d'embeddings via OllamaEmbeddings.

- ğŸ—ƒï¸ Indexation vectorielle dans Chroma, avec persistance locale.

- âš¡ Pipeline optimisÃ©e pour Ã©viter les doublons et les recalculs inutiles.

## ğŸ§¹ Nettoyage des fichiers bruts
Avant toute indexation, le code appelle une fonction clÃ© : `clean_all()`, situÃ©e dans utils/chroma/run_cleaning.py.

Cette fonction a pour rÃ´le de prÃ©parer les donnÃ©es brutes provenant de diffÃ©rents formats (CSV, Excel, PDF) en les nettoyant, puis en les exportant au format .parquet dans le dossier data/clean.

Pour cela, elle s'appuie sur trois fonctions de nettoyage spÃ©cialisÃ©es :

pour ce faire il utilise 3 fonctions de  netoyage:
- [csv_cleanet](clean_README/clean_csv.md)
- [pdf_cleaner](clean_README/clean_pdf.md)
- [xls_cleaner](clean_README/clean_xls.md)

### RÃ©sultat attendu
AprÃ¨s exÃ©cution de `clean_all()` :

Tous les fichiers bruts sont nettoyÃ©s, homogÃ©nÃ©isÃ©s et enregistrÃ©s dans :

```bash
data/clean/csv/
data/clean/xls/
data/clean/pdf/
```
Chaque fichier nettoyÃ© est ensuite converti au format `.parquet`, prÃªt Ã  Ãªtre chargÃ© dans la base vectorielle via la fonction index_documents().

### âš–ï¸ Pourquoi utiliser Polars et Pandas ensemble ?

<div style="text-align: center;">
    <img src="../img/polars_and_pandas.png" alt="Description de l'image" width="400" alignment="center">
</div>

Le projet utilise Polars comme bibliothÃ¨que principale pour le traitement des fichiers CSV, Excel et PDF convertis. Polars est plus rapide, plus lÃ©ger en mÃ©moire et hautement parallÃ©lisable par rapport Ã  Pandas, ce qui en fait un excellent choix pour les pipelines de donnÃ©es intensifs.

Cependant, Polars peut se montrer strict dans certains cas de lecture :

* Encodages ambigus ou non standards.
* DÃ©tections de types incohÃ©rentes.
* Formats Excel complexes ou mal formÃ©s.

Dans ces situations, Pandas est utilisÃ© comme solution de secours (Â« fallback Â»). Bien que moins performant, Pandas offre une tolÃ©rance plus Ã©levÃ©e aux erreurs de structure, ce qui permet de garantir que le pipeline de nettoyage reste robuste mÃªme face Ã  des fichiers rÃ©els souvent imparfaits.

â¡ï¸ En rÃ©sumÃ© :

* Polars est privilÃ©giÃ© pour la performance.
* Pandas est utilisÃ© en repli lorsquâ€™un fichier ne peut pas Ãªtre lu proprement par Polars.
* Cela permet de bÃ©nÃ©ficier des atouts de chaque bibliothÃ¨que tout en assurant la stabilitÃ© et la fiabilitÃ© du traitement des donnÃ©es.

### ğŸ’¾ Pourquoi convertir les fichiers en .parquet ?
Dans ce projet, les fichiers sources initiaux (.csv, .xls, et les contenus extraits des .pdf) sont convertis et enregistrÃ©s au format .parquet pour plusieurs raisons importantes :

* **EfficacitÃ© de stockage :** Le format Parquet est un format colonne compressÃ©, ce qui rÃ©duit significativement la taille des fichiers comparÃ© aux .csv classiques, tout en conservant toutes les informations.

* **Performance en lecture/Ã©criture :** Parquet est optimisÃ© pour un accÃ¨s rapide aux colonnes spÃ©cifiques des donnÃ©es, ce qui accÃ©lÃ¨re grandement les opÃ©rations de lecture, tri, filtrage et analyse, surtout sur de gros volumes.

* **Typage des donnÃ©es conservÃ© :** Contrairement au CSV, Parquet conserve les types de donnÃ©es (entiers, flottants, dates, etc.) de maniÃ¨re native, Ã©vitant les erreurs ou conversions rÃ©pÃ©tÃ©es lors du traitement.

* **InteropÃ©rabilitÃ© avec les outils modernes :** De nombreux outils et bibliothÃ¨ques de data science (comme Polars, Pandas, Apache Spark) supportent nativement Parquet, facilitant lâ€™intÃ©gration dans des pipelines complexes.

* **Meilleure gestion des donnÃ©es volumineuses :** Parquet est conÃ§u pour gÃ©rer des datasets volumineux de maniÃ¨re efficace, en limitant la charge mÃ©moire et en permettant un traitement parallÃ¨le.

En rÃ©sumÃ©, le passage au format .parquet rend le pipeline de traitement plus rapide, plus lÃ©ger et plus fiable, comparÃ© Ã  une gestion directe des fichiers source qui peuvent Ãªtre volumineux, mal formatÃ©s ou peu optimisÃ©s.

## ğŸš€ ExÃ©cution

```bash
python chroma_db.py
```

Cela va :
- Nettoyer les donnÃ©es brutes,
- DÃ©tecter les fichiers .parquet modifiÃ©s,
- CrÃ©er les embeddings,
- Mettre Ã  jour la base Chroma de maniÃ¨re incrÃ©mentale.

### ğŸ” Suggestion : automatisation avec CRON (optionnel)

Pour automatiser cette tÃ¢che rÃ©guliÃ¨rement (par exemple, tous les jours ou toutes les semaines), il est possible dâ€™utiliser un planificateur comme CRON.

> ğŸ’¡ Cela permettrait de maintenir la base Chroma Ã  jour automatiquement sans intervention manuelle, idÃ©alement Ã  des moments de faible affluence (comme tard le soir ou tÃ´t le matin).

Exemple de ligne CRON (exÃ©cution quotidienne Ã  2h du matin) :
```bash
0 2 * * * /usr/bin/python3 /chemin/vers/chroma_db.py >> /chemin/vers/logs/chroma_cron.log 2>&1
```

## âš™ï¸ Configuration par dÃ©faut
Cette section dÃ©finit les paramÃ¨tres et chemins par dÃ©faut utilisÃ©s tout au long du pipeline dâ€™indexation et de nettoyage :

|variable|description|
|:-------|:-------|
|`DEFAULT_CLEAN_DIR`|dossier oÃ¹ sont stockÃ©s les fichiers nettoyÃ©s et transformÃ©s (parquet notamment), issus des fichiers bruts.|
|`DEFAULT_CHROMA_DIR`|rÃ©pertoire oÃ¹ la base de donnÃ©es vectorielle Chroma est persistÃ©e, pour stocker les embeddings et documents indexÃ©s.|
|`DEFAULT_EMBEDDING_MODEL`|nom du modÃ¨le dâ€™embedding utilisÃ© pour transformer les textes en vecteurs (ici "nomic-embed-text").|
|`CACHE_FILE`|fichier JSON qui sert Ã  garder en mÃ©moire les hashs des fichiers dÃ©jÃ  traitÃ©s, afin de dÃ©tecter uniquement les changements et Ã©viter un re-traitement inutile.|
|`CHUNK_SIZE`|taille maximale (en nombre de caractÃ¨res) pour chaque segment de texte (chunk) dÃ©coupÃ© dans les documents.|
|`CHUNK_OVERLAP`|nombre de caractÃ¨res en chevauchement entre deux chunks successifs, pour assurer une continuitÃ© contextuelle.|
|`MAX_CHUNKS`|nombre maximal de chunks Ã  indexer dans une session, pour limiter la charge.|
|`BATCH_SIZE_INDEX`|taille des lots (batches) dâ€™indexation envoyÃ©s Ã  la base Chroma, pour un traitement en plusieurs passes optimisÃ©.|

## ğŸ”„ Mise Ã  jour manuelle dâ€™un seul fichier
Si on veux rÃ©indexer un fichier prÃ©cis (utile en cas de mise Ã  jour isolÃ©e) :


```python
update_file_in_index(Path("data/clean/mon_fichier.parquet"))
```

## ğŸ“Œ Remarques
Le projet est conÃ§u pour Ãªtre stateless cÃ´tÃ© modÃ¨le (on changer lâ€™embedder facilement).

La performance est optimisÃ©e par batchs (500 documents / batch) pour lâ€™indexation.

La logique est rÃ©sistante aux redÃ©marrages grÃ¢ce au cache.