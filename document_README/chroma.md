# ğŸ“š Indexation de Fichiers `.parquet` dans une Base Chroma (avec Cache et DÃ©duplication)

## ğŸ§  Objectif

Indexer automatiquement des fichiers `.parquet` dans une base vectorielle [Chroma](https://www.trychroma.com/) en Ã©vitant les doublons, en dÃ©coupant les textes et en loguant les embeddings.

---

## ğŸ”§ Pourquoi ce script ?

**Contexte :**

* Des fichiers .parquet contiennent des textes issus des donnÃ©es nettoyÃ©.

* Lâ€™objectif est de les rendre interrogeables de maniÃ¨re sÃ©mantique via une base vectorielle comme Chroma.

* Il est crucial dâ€™Ã©viter le recalcul des embeddings Ã  chaque exÃ©cution : un mÃ©canisme de cache efficace est donc mis en place pour optimiser les performances.

**Ce que fait ce code :**

| Ã‰tape            | Description                                                                |
| ---------------- | -------------------------------------------------------------------------- |
| ğŸ“ Chargement    | Ouvre dynamiquement tous les fichiers `.parquet` dans un dossier donnÃ©     |
| ğŸ§¹ Nettoyage     | Ignore les documents vides ou invalides                                    |
| ğŸ”€ DÃ©coupage     | DÃ©coupe les textes avec `RecursiveCharacterTextSplitter`                   |
| ğŸ” DÃ©duplication | Hash du contenu des chunks pour Ã©viter les doublons                        |
| ğŸ§  Embeddings    | Utilise `OllamaEmbeddings` (ex : nomic-embed-text, llama3, etc.)           |
| ğŸ§± Indexation    | Envoie les chunks dans Chroma avec des IDs uniques                         |
| ğŸ’¾ Cache         | Utilise un fichier `.json` pour ne pas retraiter deux fois un mÃªme fichier |

---

## ğŸ§¹ Nettoyage des fichiers bruts

Avant toute indexation, le code appelle une fonction clÃ© : `clean_all()`, situÃ©e dans `utils/chroma/run_cleaning.py`.

Cette fonction a pour rÃ´le de prÃ©parer les donnÃ©es brutes provenant de diffÃ©rents formats (CSV, Excel, PDF) en les nettoyant, puis en les exportant au format `.parquet` dans le dossier `data/clean`.

Pour ce faire, elle utilise 3 fonctions de nettoyage spÃ©cialisÃ©es :

* [csv\_cleaner](clean_README/clean_csv.md)
* [pdf\_cleaner](clean_README/clean_pdf.md)
* [xls\_cleaner](clean_README/clean_xls.md)

### âœ… RÃ©sultat attendu

AprÃ¨s exÃ©cution de `clean_all()` :

Tous les fichiers bruts sont nettoyÃ©s, homogÃ©nÃ©isÃ©s et enregistrÃ©s dans :

```bash
data/clean/csv/
data/clean/xls/
data/clean/pdf/
```

Chaque fichier nettoyÃ© est ensuite converti au format `.parquet`, prÃªt Ã  Ãªtre chargÃ© dans la base vectorielle via la fonction `index_documents()`.

---

### ğŸ” Pourquoi .parquet est prÃ©fÃ©rÃ© dans ce contexte
* Format binaire ultra-rapide Ã  lire/Ã©crire
    * .parquet est un format colonnaire binaire compressÃ©, contrairement Ã  .csv ou .xls qui sont textuels.
    * RÃ©sultat : Chargement et Ã©criture bien plus rapides (surtout pour des gros volumes).
    * Exemple : lire 10 fichiers .parquet peut Ãªtre 5 Ã  10 fois plus rapide quâ€™avec des .csv.

* Compression native (snappy/zstd)
    * .parquet compresse automatiquement les donnÃ©es sans perte.
    * Taille des fichiers rÃ©duite de 30% Ã  80% par rapport aux .csv ou .xls.
    * Cela accÃ©lÃ¨re aussi lâ€™I/O (moins de donnÃ©es Ã  lire depuis le disque).

* SchÃ©ma explicite et typÃ©
    * Les .csv stockent tout en texte, donc il faut souvent parser les types Ã  la main (int, float, datetime).
    * .parquet conserve les types de colonnes : plus sÃ»r, plus stable, moins dâ€™erreurs.

*  InteropÃ©rabilitÃ© & Ã©cosystÃ¨me Big Data
    * .parquet est nativement supportÃ© par Pandas, Polars, Spark, DuckDB, Dask, etc.
    * On peux facilement charger un fichier .parquet pour un traitement distribuÃ© ou vectorisÃ©.

* Nettoyage prÃ©liminaire figÃ©
Dans le pipeline :
    * On pars dâ€™un .pdf ou .xls
    * On extrais et transformes le contenu en texte nettoyÃ©
    * On le stockes une fois propre en .parquet
    * Cela permet de ne plus jamais retraiter les fichiers bruts, sauf sâ€™ils sont modifiÃ©s.
    * On nâ€™indexes que les .parquet, ce qui isole la partie texte utile.

#### âŒ InconvÃ©nients des formats bruts :

|Format|InconvÃ©nients principaux|
|---|---|
|.pdf|Non-structurÃ©, pas adaptÃ© Ã  lâ€™analyse ou lâ€™indexation directe|
|.csv|Lourd en I/O, fragile aux erreurs de format, pas typÃ©|
|.xls(x)|PropriÃ©taire, lent Ã  charger, dÃ©pend de bibliothÃ¨ques lourdes|

#### ğŸ§  Exemple dans le contexte :
Source : PDF brut = bruit, bruit visuel, sauts de ligne, erreurs dâ€™OCR

Extraction : on nettoies â†’ texte clair

Sauvegarde : .parquet = version "propre, lisible, rapide"

Indexation : .parquet â†’ LangChain â†’ chunks â†’ Chroma

#### âœ… RÃ©sumÃ©

|CritÃ¨re|.parquet|.csv|.pdf|.xls|
|---|---|---|---|---|
Vitesse de lecture|ğŸŸ¢ Excellente|ğŸ”´ Faible|ğŸ”´ TrÃ¨s faible|ğŸŸ  Moyenne
|Compression|ğŸŸ¢ Native (Snappy)|ğŸ”´ Aucune|ğŸŸ¢ possible(PDF)|ğŸŸ  Variable|
|Support des types|ğŸŸ¢ Fort|ğŸ”´ Faible|ğŸ”´ Aucun|ğŸŸ¢ Moyen|
|Lisible par machine|ğŸŸ¢ Oui|ğŸŸ¢ Oui|ğŸ”´ Non|ğŸŸ  Oui|
|Usage dans pipeline|ğŸŸ¢ IdÃ©al|ğŸŸ  Ok pour petits jeux|ğŸ”´ PrÃ©traitement requis|ğŸŸ  Ok mais lent


## âš–ï¸ Pourquoi utiliser Polars et Pandas ensemble ?

<div style="text-align: center;">
    <img src="../img/polars_and_pandas.png" alt="Polars vs Pandas" width="400" />
</div>

La pipeline dâ€™indexation repose principalement sur **Polars** pour ses performances, tout en conservant **Pandas** comme solution de repli (fallback) dans les cas plus complexes.

**Polars** est utilisÃ© en prioritÃ© car :

* ğŸš€ Il est **plus rapide** que Pandas, surtout sur les gros fichiers.
* ğŸ§  Il utilise un traitement **colonnaire** optimisÃ© et **multithreadÃ©**.
* ğŸ“‰ Il consomme **moins de mÃ©moire**.

Cependant, **Pandas** est toujours utile dans les cas suivants :

* ğŸ“„ Fichiers mal formÃ©s (Excel mal encodÃ©, CSV ambigus...).
* ğŸ§© Types de colonnes difficiles Ã  infÃ©rer automatiquement.
* ğŸ§µ Besoin de flexibilitÃ© maximale dans le parsing.

â¡ï¸ **En rÃ©sumÃ©** :

| ğŸ Lib     | Avantages                                               | RÃ´le dans le pipeline                   |
| ---------- | ------------------------------------------------------- | --------------------------------------- |
| **Polars** | Ultra rapide, typage strict, faible consommation        | Lecture principale et nettoyage initial |
| **Pandas** | TolÃ©rant aux erreurs, trÃ¨s mature, nombreuses fonctions | Fallback si Polars Ã©choue               |

ğŸ¯ **BÃ©nÃ©fice** : Cette combinaison offre le **meilleur des deux mondes** : performance et robustesse, avec une compatibilitÃ© maximale mÃªme pour des fichiers mal structurÃ©s ou issus de sources hÃ©tÃ©rogÃ¨nes.

---

## ğŸ› ï¸ Utilisation

### ğŸ Lancer lâ€™indexation principale :

```bash
python chroma_db.py
```

Cela va :

* charger tous les `.parquet` dans `data/clean` (via `glob`),
* dÃ©couper les textes en chunks,
* ignorer ceux dÃ©jÃ  vus (cache JSON),
* gÃ©nÃ©rer les embeddings,
* indexer dans la base Chroma.

### ğŸ”„ Mettre Ã  jour un fichier spÃ©cifique

```python
update_file_in_index(Path("data/clean/mon_fichier.parquet"))
```

### âš™ï¸ ParamÃ¨tres par dÃ©faut

| Variable                  | RÃ´le                                                  |
| ------------------------- | ----------------------------------------------------- |
| `DEFAULT_CLEAN_DIR`       | Dossier contenant les `.parquet` nettoyÃ©s             |
| `DEFAULT_CHROMA_DIR`      | Dossier oÃ¹ est stockÃ©e la base Chroma                 |
| `DEFAULT_EMBEDDING_MODEL` | ModÃ¨le utilisÃ© pour vectoriser (ex: nomic-embed-text) |
| `CHUNK_SIZE`              | Longueur des morceaux de texte                        |
| `CHUNK_OVERLAP`           | Chevauchement entre deux chunks                       |
| `MAX_CHUNKS`              | Nombre maximal de chunks indexÃ©s Ã  la fois            |
| `BATCH_SIZE_INDEX`        | Nombre de documents envoyÃ©s par batch Ã  Chroma        |

### ğŸ“ Cache utilisÃ©

Le cache est stockÃ© dans `index_cache.json`.

| Ã‰lÃ©ment           | RÃ´le                                        |
| ----------------- | ------------------------------------------- |
| `md5 du fichier`  | EmpÃªche de retraiter un fichier dÃ©jÃ  indexÃ© |
| `hash des chunks` | Ã‰vite dâ€™avoir deux fois le mÃªme contenu     |

### ğŸ§ª Exemple de log pour debug

```bash
âœ… Loaded 5 files.
â†’ 'ocr_rapport_2024.parquet' : 134 chunks
âœ… 127 chunks nouveaux (7 dÃ©jÃ  indexÃ©s)
ğŸ“¥ IndexÃ©s en 3 batchs (batch_size=50)
```

---

## ğŸ“Œ Remarques complÃ©mentaires

* Les IDs dans Chroma sont dÃ©rivÃ©s des hash de chaque chunk de texte (garantie dâ€™unicitÃ©).
* Les documents peuvent contenir des colonnes comme `source`, `id`, etc., conservÃ©es dans les mÃ©tadonnÃ©es.
* Le cache est persistant et robuste aux crashs.
* L'embeddage fonctionne avec nâ€™importe quel modÃ¨le LangChain-compatible (`OllamaEmbeddings`, `OpenAIEmbeddings`, etc.).
