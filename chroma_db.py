import hashlib
import time
import json
from pathlib import Path
import pandas as pd

from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter

from utils.chroma.run_cleaning import clean_all

DEFAULT_CLEAN_DIR = Path("data/clean")
DEFAULT_CHROMA_DIR = Path("chroma_db")
DEFAULT_EMBEDDING_MODEL = "nomic-embed-text"

CACHE_FILE = Path("index_cache.json")

CHUNK_SIZE = 1000
CHUNK_OVERLAP = 100
MAX_CHUNKS = 1000
BATCH_SIZE_INDEX = 500


def log_time(label: str, start: float):
     """
    Affiche le temps √©coul√© depuis un point de d√©part.

    Args:
        label (str): Nom de l'√©tape mesur√©e.
        start (float): Timestamp de d√©part.
    """
    print(f"‚è±Ô∏è {label}: {time.time() - start:.2f}s")


def generate_chunk_id(text: str) -> str:
    """
    G√©n√®re un identifiant unique (hash MD5) √† partir d'un texte.

    Args:
        text (str): Texte √† hasher.

    Returns:
        str: Hash MD5 du texte.
    """
    return hashlib.md5(text.encode("utf-8")).hexdigest()


def hash_file(path: Path) -> str:
    """
    Calcule un hash MD5 pour le contenu binaire d'un fichier.

    Args:
        path (Path): Chemin du fichier.

    Returns:
        str: Hash MD5 du fichier.
    """
    h = hashlib.md5()
    with open(path, "rb") as f:
        while chunk := f.read(8192):
            h.update(chunk)
    return h.hexdigest()


def load_cache() -> dict:
    """
    Charge le cache de hachage depuis un fichier JSON.

    Returns:
        dict: Cache contenant les hachages pr√©c√©dents des fichiers.
    """
    if CACHE_FILE.exists():
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}


def save_cache(cache: dict):
    """
    Sauvegarde le cache de hachage dans un fichier JSON.

    Args:
        cache (dict): Dictionnaire de hachages √† sauvegarder.
    """
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(cache, f, indent=2, ensure_ascii=False)


def load_parquet_documents(clean_dir: Path, changed_files: set[str]) -> list[Document]:
    """
    Charge les fichiers .parquet modifi√©s et cr√©e des documents LangChain.

    Args:
        clean_dir (Path): R√©pertoire contenant les fichiers nettoy√©s.
        changed_files (set[str]): Fichiers √† recharger (modifi√©s).

    Returns:
        list[Document]: Documents extraits √† partir des fichiers .parquet.
    """
    documents = []
    for file in clean_dir.rglob("*.parquet"):
        if file.name not in changed_files:
            # On ignore les fichiers non modifi√©s
            continue
        df = pd.read_parquet(file)
        for _, row in df.iterrows():
            text = " | ".join(str(value) for value in row.values if pd.notna(value)).strip()
            if text:
                documents.append(Document(
                    page_content=text,
                    metadata={"source_file": file.name}
                ))
    return documents


def get_existing_ids(chroma_dir: Path) -> set[str]:
    """
    R√©cup√®re les IDs de documents d√©j√† index√©s dans la base Chroma.

    Args:
        chroma_dir (Path): R√©pertoire de persistance de la base Chroma.

    Returns:
        set[str]: Ensemble des identifiants index√©s.
    """
    if not chroma_dir.exists():
        return set()
    try:
        db = Chroma(persist_directory=str(chroma_dir), embedding_function=None)
        return set(db.get()['ids'])
    except Exception as e:
        print(f"‚ö†Ô∏è Impossible de r√©cup√©rer les IDs existants : {e}")
        return set()


def index_documents(
    clean_dir: Path = DEFAULT_CLEAN_DIR,
    chroma_dir: Path = DEFAULT_CHROMA_DIR,
    embedding_model: str = DEFAULT_EMBEDDING_MODEL,
    embedding=None,
) -> dict | None:
    """
    Indexe les documents nettoy√©s en cr√©ant des embeddings pour Chroma.

    Args:
        clean_dir (Path): R√©pertoire des fichiers nettoy√©s.
        chroma_dir (Path): R√©pertoire de la base Chroma.
        embedding_model (str): Nom du mod√®le d'embedding √† utiliser.
        embedding: Instance d'un mod√®le d'embedding, facultatif.

    Returns:
        dict | None: R√©sum√© de l'indexation, ou None si rien √† faire.
    """
    global_start = time.time()

    print("üîç Chargement du cache de hash fichiers...")
    cache = load_cache()

    print("üì• Recherche des fichiers .parquet modifi√©s ou nouveaux...")
    changed_files = set()
    for file in clean_dir.rglob("*.parquet"):
        current_hash = hash_file(file)
        cached_hash = cache.get(file.name)
        if current_hash != cached_hash:
            print(f"üÜï Fichier modifi√© ou nouveau d√©tect√©: {file.name}")
            changed_files.add(file.name)
            cache[file.name] = current_hash

    if not changed_files:
        print("‚úÖ Aucun fichier modifi√©. Pas besoin de r√©indexer.")
        return None

    print(f"üì• Chargement des documents des fichiers modifi√©s ({len(changed_files)})...")
    start = time.time()
    raw_docs = load_parquet_documents(clean_dir, changed_files)
    log_time("Chargement", start)
    print(f"‚úÖ {len(raw_docs)} documents bruts charg√©s.")

    if not raw_docs:
        print("‚ö†Ô∏è Aucun document √† traiter apr√®s filtre. Fin de la pipeline.")
        return None

    print("üßπ D√©duplication des documents...")
    start = time.time()
    seen = set()
    unique_docs = []
    for doc in raw_docs:
        h = generate_chunk_id(doc.page_content)
        if h not in seen:
            seen.add(h)
            unique_docs.append(doc)
    log_time("D√©duplication", start)
    print(f"‚úÖ {len(unique_docs)} documents uniques apr√®s d√©duplication.")

    print("üî™ D√©coupage des documents en chunks...")
    start = time.time()
    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    chunks = []
    for doc in unique_docs:
        content = doc.page_content.strip()
        if len(content) < CHUNK_SIZE:
            if len(content) > 50:
                chunks.append(doc)
        else:
            chunks.extend(splitter.split_documents([doc]))
    log_time("D√©coupage", start)
    print(f"‚úÖ {len(chunks)} chunks g√©n√©r√©s.")

    print("üÜî Attribution des IDs aux chunks...")
    for chunk in chunks:
        chunk.metadata["id"] = generate_chunk_id(chunk.page_content)

    print("üìÇ R√©cup√©ration des IDs d√©j√† index√©s dans Chroma...")
    existing_ids = get_existing_ids(chroma_dir)

    new_chunks = [chunk for chunk in chunks if chunk.metadata["id"] not in existing_ids]
    print(f"üÜï {len(new_chunks)} nouveaux chunks √† indexer.")

    if not new_chunks:
        print("‚úÖ Aucun nouveau chunk √† indexer. Fin de la pipeline.")
        return {
            "raw_docs": len(raw_docs),
            "unique_docs": len(unique_docs),
            "chunks": len(chunks),
            "indexed": 0,
        }

    new_chunks = new_chunks[:MAX_CHUNKS]

    print("üß† Indexation dans Chroma (par batch)...")
    start = time.time()
    embedding = embedding or OllamaEmbeddings(model=embedding_model)
    vectordb = Chroma(persist_directory=str(chroma_dir), embedding_function=embedding)

    total = len(new_chunks)
    batches = [new_chunks[i:i + BATCH_SIZE_INDEX] for i in range(0, total, BATCH_SIZE_INDEX)]

    for i, batch in enumerate(batches, 1):
        batch_ids = [chunk.metadata["id"] for chunk in batch]
        try:
            vectordb.add_documents(batch, ids=batch_ids)
            print(f"‚úÖ Batch {i}/{len(batches)} index√©.")
        except Exception as e:
            print(f"‚ùå Erreur lors de l‚Äôindexation batch {i}: {e}")

    log_time("Pipeline compl√®te", global_start)
    print("‚úÖ Mise √† jour de Chroma termin√©e avec succ√®s.")

    # Sauvegarde du cache mis √† jour
    save_cache(cache)

    return {
        "raw_docs": len(raw_docs),
        "unique_docs": len(unique_docs),
        "chunks": len(chunks),
        "indexed": len(new_chunks),
    }

def update_file_in_index(
    file_path: Path,
    chroma_dir: Path = DEFAULT_CHROMA_DIR,
    embedding_model: str = "nomic-embed-text",
):
    """
    Met √† jour la base Chroma pour un seul fichier .parquet donn√©.

    Args:
        file_path (Path): Chemin vers le fichier √† indexer.
        chroma_dir (Path): R√©pertoire de la base Chroma.
        embedding_model (str): Nom du mod√®le d'embedding.
    """
    # Charger le fichier (exemple CSV/Parquet)
    import pandas as pd
    df = pd.read_parquet(file_path)
    
    # Cr√©er documents
    documents = []
    for _, row in df.iterrows():
        text = " | ".join(str(v) for v in row.values if pd.notna(v)).strip()
        if text:
            documents.append(Document(page_content=text, metadata={"source_file": file_path.name}))

    # D√©couper en chunks
    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    chunks = []
    for doc in documents:
        if len(doc.page_content) < CHUNK_SIZE:
            chunks.append(doc)
        else:
            chunks.extend(splitter.split_documents([doc]))
    
    # Calculer IDs des chunks
    for chunk in chunks:
        chunk.metadata["id"] = generate_chunk_id(chunk.page_content)
    
    # Charger la base Chroma existante
    embedding = OllamaEmbeddings(model=embedding_model)
    vectordb = Chroma(persist_directory=str(chroma_dir), embedding_function=embedding)
    
    # R√©cup√©rer IDs d√©j√† index√©s
    existing_ids = set(vectordb.get()['ids'])
    
    # Filtrer les chunks d√©j√† index√©s
    new_chunks = [chunk for chunk in chunks if chunk.metadata["id"] not in existing_ids]
    
    if not new_chunks:
        print("Aucun nouveau chunk √† indexer.")
        return
    
    # Ajouter les nouveaux chunks √† la base
    new_ids = [chunk.metadata["id"] for chunk in new_chunks]
    vectordb.add_documents(new_chunks, ids=new_ids)
    
    print(f"{len(new_chunks)} chunks ajout√©s √† la base.")

    
    
if __name__ == "__main__":
    # Nettoyage des donn√©es brutes vers `data/clean`
    clean_all()

    # Cr√©ation de la base vectorielle dans `data/vectorstore`
    index_documents()