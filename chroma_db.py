import hashlib
import time
import json
from pathlib import Path
import pandas as pd

from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter

from utils.chroma.run_cleaning import clean_all

DEFAULT_CLEAN_DIR = Path("data/clean")
DEFAULT_CHROMA_DIR = Path("chroma_db")
DEFAULT_EMBEDDING_MODEL = "nomic-embed-text"

CACHE_FILE = Path("index_cache.json")

CHUNK_SIZE = 1000
CHUNK_OVERLAP = 100
MAX_CHUNKS = 1000
BATCH_SIZE_INDEX = 500


def log_time(label: str, start: float):
    """
    Affiche le temps Ã©coulÃ© depuis un point de dÃ©part.

    Args:
        label (str): Nom de l'Ã©tape mesurÃ©e.
        start (float): Timestamp de dÃ©part.
    """
    print(f"â±ï¸ {label}: {time.time() - start:.2f}s")


def generate_chunk_id(text: str) -> str:
    """
    GÃ©nÃ¨re un identifiant unique (hash MD5) Ã  partir d'un texte.

    Args:
        text (str): Texte Ã  hasher.

    Returns:
        str: Hash MD5 du texte.
    """
    return hashlib.md5(text.encode("utf-8")).hexdigest()


def hash_file(path: Path) -> str:
    """
    Calcule un hash MD5 pour le contenu binaire d'un fichier.

    Args:
        path (Path): Chemin du fichier.

    Returns:
        str: Hash MD5 du fichier.
    """
    h = hashlib.md5()
    with open(path, "rb") as f:
        while chunk := f.read(8192):
            h.update(chunk)
    return h.hexdigest()


def load_cache() -> dict:
    """
    Charge le cache de hachage depuis un fichier JSON.

    Returns:
        dict: Cache contenant les hachages prÃ©cÃ©dents des fichiers.
    """
    if CACHE_FILE.exists():
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}


def save_cache(cache: dict):
    """
    Sauvegarde le cache de hachage dans un fichier JSON.

    Args:
        cache (dict): Dictionnaire de hachages Ã  sauvegarder.
    """
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(cache, f, indent=2, ensure_ascii=False)


def load_parquet_documents(clean_dir: Path, changed_files: set[str]) -> list[Document]:
    """
    Charge les fichiers .parquet modifiÃ©s et crÃ©e des documents LangChain.

    Args:
        clean_dir (Path): RÃ©pertoire contenant les fichiers nettoyÃ©s.
        changed_files (set[str]): Fichiers Ã  recharger (modifiÃ©s).

    Returns:
        list[Document]: Documents extraits Ã  partir des fichiers .parquet.
    """
    documents = []
    for file in clean_dir.rglob("*.parquet"):
        if file.name not in changed_files:
            # On ignore les fichiers non modifiÃ©s
            continue
        df = pd.read_parquet(file)
        for _, row in df.iterrows():
            text = " | ".join(str(value) for value in row.values if pd.notna(value)).strip()
            if text:
                documents.append(Document(
                    page_content=text,
                    metadata={"source_file": file.name}
                ))
    return documents


def get_existing_ids(chroma_dir: Path) -> set[str]:
    """
    RÃ©cupÃ¨re les IDs de documents dÃ©jÃ  indexÃ©s dans la base Chroma.

    Args:
        chroma_dir (Path): RÃ©pertoire de persistance de la base Chroma.

    Returns:
        set[str]: Ensemble des identifiants indexÃ©s.
    """
    if not chroma_dir.exists():
        return set()
    try:
        db = Chroma(persist_directory=str(chroma_dir), embedding_function=None)
        return set(db.get()['ids'])
    except Exception as e:
        print(f"âš ï¸ Impossible de rÃ©cupÃ©rer les IDs existants : {e}")
        return set()


def index_documents(
    clean_dir: Path = DEFAULT_CLEAN_DIR,
    chroma_dir: Path = DEFAULT_CHROMA_DIR,
    embedding_model: str = DEFAULT_EMBEDDING_MODEL,
    embedding=None,
    max_retries: int = 3,          # nombre max de tentatives par batch
    retry_delay: float = 2.0,      # dÃ©lai entre retries en secondes
    batch_delay: float = 1.0       # dÃ©lai entre batches en secondes
) -> dict | None:
    global_start = time.time()

    print("ğŸ” Chargement du cache de hash fichiers...")
    cache = load_cache()

    print("ğŸ“¥ Recherche des fichiers .parquet modifiÃ©s ou nouveaux...")
    changed_files = set()
    for file in clean_dir.rglob("*.parquet"):
        current_hash = hash_file(file)
        cached_hash = cache.get(file.name)
        if current_hash != cached_hash:
            print(f"ğŸ†• Fichier modifiÃ© ou nouveau dÃ©tectÃ©: {file.name}")
            changed_files.add(file.name)
            cache[file.name] = current_hash

    if not changed_files:
        print("âœ… Aucun fichier modifiÃ©. Pas besoin de rÃ©indexer.")
        return None

    print(f"ğŸ“¥ Chargement des documents des fichiers modifiÃ©s ({len(changed_files)})...")
    start = time.time()
    raw_docs = load_parquet_documents(clean_dir, changed_files)
    log_time("Chargement", start)
    print(f"âœ… {len(raw_docs)} documents bruts chargÃ©s.")

    if not raw_docs:
        print("âš ï¸ Aucun document Ã  traiter aprÃ¨s filtre. Fin de la pipeline.")
        return None

    print("ğŸ§¹ DÃ©duplication des documents...")
    start = time.time()
    seen = set()
    unique_docs = []
    for doc in raw_docs:
        h = generate_chunk_id(doc.page_content)
        if h not in seen:
            seen.add(h)
            unique_docs.append(doc)
    log_time("DÃ©duplication", start)
    print(f"âœ… {len(unique_docs)} documents uniques aprÃ¨s dÃ©duplication.")

    print("ğŸ”ª DÃ©coupage des documents en chunks...")
    start = time.time()
    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    chunks = []
    for doc in unique_docs:
        content = doc.page_content.strip()
        if len(content) < CHUNK_SIZE:
            if len(content) > 50:
                chunks.append(doc)
        else:
            chunks.extend(splitter.split_documents([doc]))
    log_time("DÃ©coupage", start)
    print(f"âœ… {len(chunks)} chunks gÃ©nÃ©rÃ©s.")

    print("ğŸ†” Attribution des IDs aux chunks...")
    for chunk in chunks:
        chunk.metadata["id"] = generate_chunk_id(chunk.page_content)

    print("ğŸ“‚ RÃ©cupÃ©ration des IDs dÃ©jÃ  indexÃ©s dans Chroma...")
    existing_ids = get_existing_ids(chroma_dir)

    new_chunks = [chunk for chunk in chunks if chunk.metadata["id"] not in existing_ids]
    print(f"ğŸ†• {len(new_chunks)} nouveaux chunks Ã  indexer.")

    if not new_chunks:
        print("âœ… Aucun nouveau chunk Ã  indexer. Fin de la pipeline.")
        return {
            "raw_docs": len(raw_docs),
            "unique_docs": len(unique_docs),
            "chunks": len(chunks),
            "indexed": 0,
        }

    new_chunks = new_chunks[:MAX_CHUNKS]

    print("ğŸ§  Indexation dans Chroma (par batch)...")
    start = time.time()
    embedding = embedding or OllamaEmbeddings(model=embedding_model)
    vectordb = Chroma(persist_directory=str(chroma_dir), embedding_function=embedding)

    total = len(new_chunks)
    batches = [new_chunks[i:i + BATCH_SIZE_INDEX] for i in range(0, total, BATCH_SIZE_INDEX)]

    successful_index = False

    for i, batch in enumerate(batches, 1):
        batch_ids = [chunk.metadata["id"] for chunk in batch]
        attempt = 0
        while attempt < max_retries:
            try:
                vectordb.add_documents(batch, ids=batch_ids)
                print(f"âœ… Batch {i}/{len(batches)} indexÃ© (tentative {attempt + 1}).")
                successful_index = True
                break
            except Exception as e:
                attempt += 1
                print(f"âŒ Erreur lors de lâ€™indexation batch {i} (tentative {attempt}): {e}")
                if attempt < max_retries:
                    print(f"ğŸ”„ Nouvelle tentative dans {retry_delay}s...")
                    time.sleep(retry_delay)
                else:
                    print(f"âš ï¸ Ã‰chec dÃ©finitif du batch {i} aprÃ¨s {max_retries} tentatives.")
        time.sleep(batch_delay)

    log_time("Pipeline complÃ¨te", global_start)

    if successful_index:
        save_cache(cache)
        print("âœ… Mise Ã  jour de Chroma et cache terminÃ©e avec succÃ¨s.")
    else:
        print("âš ï¸ Aucun batch nâ€™a Ã©tÃ© indexÃ© avec succÃ¨s. Le cache nâ€™a pas Ã©tÃ© mis Ã  jour.")

    return {
        "raw_docs": len(raw_docs),
        "unique_docs": len(unique_docs),
        "chunks": len(chunks),
        "indexed": len(new_chunks) if successful_index else 0,
    }
    
        
def update_file_in_index(
    file_path: Path,
    chroma_dir: Path = DEFAULT_CHROMA_DIR,
    embedding_model: str = "nomic-embed-text",
):
    """
    Met Ã  jour la base Chroma pour un seul fichier .parquet donnÃ©.

    Args:
        file_path (Path): Chemin vers le fichier Ã  indexer.
        chroma_dir (Path): RÃ©pertoire de la base Chroma.
        embedding_model (str): Nom du modÃ¨le d'embedding.
    """
    # Charger le fichier (exemple CSV/Parquet)
    import pandas as pd
    df = pd.read_parquet(file_path)
    
    # CrÃ©er documents
    documents = []
    for _, row in df.iterrows():
        text = " | ".join(str(v) for v in row.values if pd.notna(v)).strip()
        if text:
            documents.append(Document(page_content=text, metadata={"source_file": file_path.name}))

    # DÃ©couper en chunks
    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    chunks = []
    for doc in documents:
        if len(doc.page_content) < CHUNK_SIZE:
            chunks.append(doc)
        else:
            chunks.extend(splitter.split_documents([doc]))
    
    # Calculer IDs des chunks
    for chunk in chunks:
        chunk.metadata["id"] = generate_chunk_id(chunk.page_content)
    
    # Charger la base Chroma existante
    embedding = OllamaEmbeddings(model=embedding_model)
    vectordb = Chroma(persist_directory=str(chroma_dir), embedding_function=embedding)
    
    # RÃ©cupÃ©rer IDs dÃ©jÃ  indexÃ©s
    existing_ids = set(vectordb.get()['ids'])
    
    # Filtrer les chunks dÃ©jÃ  indexÃ©s
    new_chunks = [chunk for chunk in chunks if chunk.metadata["id"] not in existing_ids]
    
    if not new_chunks:
        print("Aucun nouveau chunk Ã  indexer.")
        return
    
    # Ajouter les nouveaux chunks Ã  la base
    new_ids = [chunk.metadata["id"] for chunk in new_chunks]
    vectordb.add_documents(new_chunks, ids=new_ids)
    
    print(f"{len(new_chunks)} chunks ajoutÃ©s Ã  la base.")

    
    
if __name__ == "__main__":
    # Nettoyage des donnÃ©es brutes vers `data/clean`
    clean_all()

    # CrÃ©ation de la base vectorielle dans `data/vectorstore`
    index_documents()